import os
import subprocess
import sys
import time
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path
from typing import Any, Tuple

import requests
import urllib3
from dotenv import load_dotenv

from e2e.runner.env import (
    apply_gateway_env_from_container,
    apply_ports_to_env,
    apply_proxy_env,
    calculate_runtime_env,
    ensure_firecracker_node_up,
    load_ports,
)
from e2e.runner.utils import (
    BRAND_SLUG,
    E2E_STATE_ROOT,
    PROJECT_ROOT,
    env_key,
    run_esb,
)

# Terminal colors for parallel output
COLORS = [
    "\033[36m",  # Cyan
    "\033[32m",  # Green
    "\033[34m",  # Blue
    "\033[35m",  # Magenta
    "\033[33m",  # Yellow
]
COLOR_RESET = "\033[0m"


def thorough_cleanup(env_name: str):
    """Exhaustively remove Docker resources associated with an environment."""
    project_label = f"{BRAND_SLUG}-{env_name}"

    # 1. Containers
    container_filters = [
        f"name={env_name}",
        f"label=com.docker.compose.project={project_label}",
    ]
    for filt in container_filters:
        result = subprocess.run(
            ["docker", "ps", "-aq", "--filter", filt],
            capture_output=True,
            text=True,
        )
        container_ids = [cid.strip() for cid in result.stdout.split() if cid.strip()]
        if container_ids:
            print(f"  â€¢ Removing containers for {env_name} ({filt})...")
            subprocess.run(["docker", "rm", "-f"] + container_ids, capture_output=True)

    # 2. Networks
    network_filters = [
        f"label=com.docker.compose.project={project_label}",
        f"name={project_label}-external",
        f"name={project_label}_default",
    ]
    for filt in network_filters:
        result = subprocess.run(
            ["docker", "network", "ls", "-q", "--filter", filt],
            capture_output=True,
            text=True,
        )
        network_ids = [nid.strip() for nid in result.stdout.split() if nid.strip()]
        if network_ids:
            print(f"  â€¢ Removing networks for {env_name} ({filt})...")
            # Networks might still be in use if some containers weren't properly removed
            subprocess.run(["docker", "network", "rm"] + network_ids, capture_output=True)

    # 3. Volumes
    # We check by label AND by name prefix for maximum safety
    volume_filters = [
        f"label=com.docker.compose.project={project_label}",
        f"name={project_label}_",
    ]
    seen_volumes = set()
    for filt in volume_filters:
        result = subprocess.run(
            ["docker", "volume", "ls", "-q", "--filter", filt],
            capture_output=True,
            text=True,
        )
        volume_ids = [vid.strip() for vid in result.stdout.split() if vid.strip()]
        to_remove = [v for v in volume_ids if v not in seen_volumes]
        if to_remove:
            print(f"  â€¢ Removing volumes for {env_name} ({filt})...")
            subprocess.run(["docker", "volume", "rm"] + to_remove, capture_output=True)
            seen_volumes.update(to_remove)
    # Note: Image pruning removed from per-scenario cleanup.
    # 'docker image prune' is a global operation that interferes with parallel execution.
    # Run it manually or via a post-test cleanup script instead.


def warmup_environment(env_scenarios: dict, matrix: list[dict], args):
    """
    Perform global reset and warm-up actions.
    This includes cleaning up old artifacts and registering the ESB project
    in the global config before parallel execution.
    """
    # Note: Thorough cleanup is now handled per-scenario in run_scenario
    # to support targeted resets (e.g. when --profile is specified).

    # 2. Build environment list from matrix (not generator.yml - that's generated by esb project add)
    active_envs = list(env_scenarios.keys())
    if not active_envs:
        print("[ERROR] No active environments in matrix.")
        sys.exit(1)

    # Build env list with modes from ALL matrix entries to ensure generator.yml
    # remains consistent even when running a single profile.
    env_list_parts = []
    for entry in matrix:
        env_name = entry.get("esb_env")
        if not env_name:
            continue
        # Derive mode from env_file name (e.g., .env.docker -> docker)
        env_file = entry.get("env_file", "")
        mode = (
            "containerd"
            if "containerd" in env_file
            else "firecracker"
            if "firecracker" in env_file
            else "docker"
        )
        env_list_parts.append(f"{env_name}:{mode}")

    env_list = ",".join(env_list_parts)

    # 3. Register ESB project (this generates generator.yml)
    print(f"\n[INIT] Registering ESB project for: {', '.join(active_envs)}")
    # Use the default E2E template for registration
    esb_template = "e2e/fixtures/template.yaml"
    if not (PROJECT_ROOT / esb_template).exists():
        # Fallback or error if template missing, mostly relevant for fresh clones
        pass

    # We use subprocess directly to call go run ... project add
    # Assuming run_esb helper logic or direct call
    # Here we replicate the call from original script
    # Register ESB project (this generates generator.yml)
    project_name = os.environ.get(env_key("PROJECT"), BRAND_SLUG)
    run_esb(
        [
            "project",
            "add",
            ".",
            "--template",
            str(PROJECT_ROOT / esb_template),
            "--env",
            env_list,
            "--name",
            project_name,
        ]
    )


def run_scenario(args, scenario):
    """Run a single scenario."""
    # 0. Resolve scenario-specific parameters once
    env_name = scenario.get("esb_env", os.environ.get(env_key("ENV"), "e2e-docker"))
    raw_env_file = scenario.get("env_file")
    env_file = str((PROJECT_ROOT / raw_env_file).absolute()) if raw_env_file else None
    project_name = scenario.get("esb_project", BRAND_SLUG)
    do_reset = scenario.get("perform_reset", args.reset)
    do_build = scenario.get("perform_build", args.build)
    build_only = scenario.get("build_only", False)
    env_vars_override = scenario.get("env_vars", {})

    # 0.1 Set ESB variables for resolution and safety
    # We set these in os.environ so the CLI doesn't prompt even if .env fails.
    os.environ[env_key("PROJECT")] = project_name
    os.environ[env_key("ENV")] = env_name
    os.environ[env_key("HOME")] = str((E2E_STATE_ROOT / env_name).absolute())

    # Load Scenario-Specific Env File (Required for isolation)
    if env_file:
        p = Path(env_file)
        if p.exists():
            load_dotenv(p, override=True)
            print(f"Loaded scenario environment from: {p}")
        else:
            print(f"Warning: Scenario environment file not found: {p}")
    else:
        print("Warning: No env_file specified for this scenario. Operating with system env only.")

    # 2.5 Inject Proxy Settings
    apply_proxy_env()

    # 3. Reload env vars into a dict for passing to subprocess (pytest)
    env = os.environ.copy()

    # Capture calculated values for convenience
    env["GATEWAY_PORT"] = env.get(env_key("PORT_GATEWAY_HTTPS"), "443")
    env["VICTORIALOGS_PORT"] = env.get(env_key("PORT_VICTORIALOGS"), "9428")
    env["GATEWAY_URL"] = f"https://localhost:{env['GATEWAY_PORT']}"
    env["VICTORIALOGS_URL"] = f"http://localhost:{env['VICTORIALOGS_PORT']}"
    env["AGENT_GRPC_ADDRESS"] = f"localhost:{env.get(env_key('PORT_AGENT_GRPC'), '50051')}"
    env[env_key("PROJECT_NAME")] = f"{project_name}-{env_name}"

    # Merge scenario-specific environment variables
    env.update(env_vars_override)

    ensure_firecracker_node_up()

    # 1.5 Calculate Runtime Env (needed for reset/build too)
    mode = scenario.get("mode", "docker")
    runtime_env = calculate_runtime_env(project_name, env_name, mode, env_file)

    did_up = False
    try:
        # 2. Reset / Build
        if do_reset:
            print(f"âžœ Resetting environment: {env_name}")
            # 2.1 Thorough Docker cleanup for this environment
            thorough_cleanup(env_name)

            # 2.2 Clean artifact directory for this environment
            env_state_dir = E2E_STATE_ROOT / env_name
            if env_state_dir.exists():
                print(f"  â€¢ Cleaning artifact directory: {env_state_dir}")
                import shutil

                shutil.rmtree(env_state_dir)

            # Re-generate configurations and build images via ESB build
            # IMPORTANT: Pass runtime_env so (branding) is respected!
            # We explicitly set ESB_REPO (BRAND_REPO) to point to fixtures so the CLI finds generator.yml
            # without changing the process CWD (which is destructive/confusing).
            build_env = runtime_env.copy()
            build_env[env_key("REPO")] = str(PROJECT_ROOT / "e2e" / "fixtures")

            run_esb(
                ["build", "--no-cache"],
                env_file=env_file,
                verbose=args.verbose,
                env=build_env,
            )

            if build_only:
                return True

            # Manual orchestration will handle starting the services in Step 3
            did_up = False
        elif do_build:
            if not args.verbose:
                print(f"âžœ Building environment: {env_name}... ", end="", flush=True)
            else:
                print(f"âžœ Building environment: {env_name}")
            run_esb(["build", "--no-cache"], env_file=env_file, verbose=args.verbose)
            if not args.verbose:
                print("Done")
        else:
            if not args.verbose:
                print(f"âžœ Preparing environment: {env_name}... ", end="", flush=True)
            else:
                print(f"âžœ Preparing environment: {env_name}")
            # Ensure services are stopped before starting (without destroying data)
            run_esb(["stop"], check=True, env_file=env_file, verbose=args.verbose)
            run_esb(["build"], env_file=env_file, verbose=args.verbose)
            if not args.verbose:
                print("Done")

        # If build_only, skip UP and tests
        if build_only:
            return

        # 3. UP (Manual Orchestration)
        if not did_up:
            # Critical: Override PROJECT_NAME to include env suffix for isolation (e.g. esb-e2e-docker)
            # This matches the logic in the Go CLI builder and ensures container names are unique.
            runtime_env["PROJECT_NAME"] = f"{project_name}-{env_name}"

            # Merge with existing system/process env to ensure PATH etc are preserved
            compose_env = os.environ.copy()
            compose_env.update(runtime_env)
            compose_env.update(env_vars_override)

            # Target the static docker-compose file in project root
            # per user instruction: /home/akira/esb/docker-compose.{mode}.yml
            compose_file_path = PROJECT_ROOT / f"docker-compose.{mode}.yml"
            if not compose_file_path.exists():
                print(f"[WARN] Static compose file not found at {compose_file_path}.")
                # Fail fast as this is a configuration error
                raise FileNotFoundError(f"Static compose file not found: {compose_file_path}")

            compose_cmd = [
                "docker",
                "compose",
                "--project-name",
                f"{BRAND_SLUG}-{env_name}",
                "--file",
                str(compose_file_path),
                "up",
                "--detach",
            ]

            if args.verbose:
                print(f"Running: {' '.join(compose_cmd)}")

            subprocess.run(compose_cmd, check=True, env=compose_env)

            # Sync generates ports.json and provisions resources
            run_esb(["sync"], env_file=env_file, verbose=args.verbose, env=runtime_env)

            # Wait for Gateway readiness (parity with legacy esb up)
            wait_for_gateway(env_name, verbose=args.verbose)

        # 3.5 Load dynamic ports from ports.json (created by esb up)
        ports = load_ports(env_name)
        if ports:
            apply_ports_to_env(ports)

            # Update env dict for pytest subprocess
            for k, v in ports.items():
                env[k] = str(v)

            env["GATEWAY_PORT"] = str(
                ports.get(env_key("PORT_GATEWAY_HTTPS"), env.get("GATEWAY_PORT", "443"))
            )
            env["VICTORIALOGS_PORT"] = str(
                ports.get(env_key("PORT_VICTORIALOGS"), env.get("VICTORIALOGS_PORT", "9428"))
            )
            env["GATEWAY_URL"] = f"https://localhost:{env['GATEWAY_PORT']}"
            env["VICTORIALOGS_URL"] = f"http://localhost:{env['VICTORIALOGS_PORT']}"
            agent_key = env_key("PORT_AGENT_GRPC")
            if agent_key in ports:
                env["AGENT_GRPC_ADDRESS"] = f"localhost:{ports[agent_key]}"

            agent_metrics_key = env_key("PORT_AGENT_METRICS")
            if agent_metrics_key in ports:
                env["AGENT_METRICS_PORT"] = str(ports[agent_metrics_key])
                env["AGENT_METRICS_URL"] = f"http://localhost:{ports[agent_metrics_key]}"

        apply_gateway_env_from_container(env, env_file)

        # 4. Run Tests
        if not scenario["targets"]:
            # No test targets specified, skip test execution
            return

        print(f"\\n=== Running Tests for {scenario['name']} ===\n")

        pytest_cmd = [sys.executable, "-m", "pytest"] + scenario["targets"] + ["-v"]

        # Excludes
        for excl in scenario["exclude"]:
            pytest_cmd.extend(["--ignore", excl])

        # Pass the full env with calculated ports to pytest
        result = subprocess.run(pytest_cmd, cwd=PROJECT_ROOT, check=False, env=env)

        if result.returncode != 0:
            sys.exit(result.returncode)

    except subprocess.CalledProcessError as e:
        print(f"Error executing command: {e}")
        sys.exit(1)
    except RuntimeError as e:
        print(f"Error executing command: {e}")
        sys.exit(1)

    finally:
        if args.cleanup:
            if not args.verbose:
                print(f"âžœ Cleaning up environment: {env_name}... ", end="", flush=True)
            else:
                print(f"âžœ Cleaning up environment: {env_name}")
            run_esb(["down"], env_file=scenario.get("env_file"), verbose=args.verbose)
            if not args.verbose:
                print("Done")


def run_profile_subprocess(
    profile_name: str,
    cmd: list[str],
    color_code: str = "",
    verbose: bool = False,
    label_width: int = 0,
) -> Tuple[int, str]:
    """Run a profile in a subprocess and stream output with prefix."""
    label = f"[{profile_name}]"
    if label_width > 0:
        label = label.ljust(label_width)
    prefix = f"{color_code}{label}{COLOR_RESET}"

    # Inject flags to force non-interactive behavior
    env = os.environ.copy()
    env["TERM"] = "dumb"
    env[env_key("INTERACTIVE")] = "0"
    env["E2E_WORKER"] = "1"

    process = subprocess.Popen(
        cmd,
        cwd=PROJECT_ROOT,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        stdin=subprocess.DEVNULL,
        text=True,
        bufsize=1,  # Line buffered
        env=env,
    )

    output_lines = []
    tests_started = False
    in_special_block = False
    last_line_was_blank = True
    early_failure = False
    early_failure_patterns = (
        "Error executing command:",
        "ERROR: failed to build",
        "failed to solve:",
    )

    # Read output line by line as it becomes available
    try:
        if process.stdout:
            for line in iter(process.stdout.readline, ""):
                clean_line = line.rstrip()
                if clean_line:
                    if "test session starts" in clean_line:
                        tests_started = True

                    # Detect special info blocks (Auth credentials and Discovered Ports)
                    # Starts with Key or Plug emoji
                    is_special_header = clean_line.startswith("ðŸ”‘") or clean_line.startswith("ðŸ”Œ")
                    if is_special_header:
                        in_special_block = True

                    should_print = (
                        verbose or tests_started or clean_line.startswith("âžœ") or in_special_block
                    )

                    if should_print:
                        print(f"{prefix} {clean_line}", flush=True)
                        last_line_was_blank = False

                    # End of special block if we encounter a new progress line
                    # or if the line is not indented (and not the header itself)
                    if in_special_block and not is_special_header:
                        if clean_line.startswith("âžœ") or not clean_line.startswith(" "):
                            in_special_block = False

                    if not tests_started and any(
                        pat in clean_line for pat in early_failure_patterns
                    ):
                        early_failure = True
                        print(f"{prefix} âžœ Build failed; stopping this environment.", flush=True)
                        process.terminate()
                        break
                else:
                    # Empty line terminates a block
                    if in_special_block:
                        in_special_block = False

                    # Preserve blank lines for structure (e.g. before BlockStart)
                    if not last_line_was_blank:
                        print(prefix, flush=True)
                        last_line_was_blank = True

                output_lines.append(line)

    except Exception as e:
        print(f"{prefix} Error reading output: {e}")

    try:
        returncode = process.wait(timeout=15)
    except subprocess.TimeoutExpired:
        if early_failure:
            process.kill()
        returncode = process.wait()

    if returncode != 0 and not verbose and not tests_started:
        print(f"{prefix} âžœ Subprocess failed before tests started. Printing cached logs...\n")
        for line in output_lines:
            print(f"{prefix} {line.rstrip()}", flush=True)

    # Write output to a log file for debugging
    log_file = PROJECT_ROOT / "e2e" / f".parallel-{profile_name}.log"
    with open(log_file, "w", encoding="utf-8") as f:
        f.write(f"=== {profile_name} combined output ===\n")
        f.writelines(output_lines)

    return returncode, "".join(output_lines)


def run_profiles_with_executor(
    env_scenarios: dict[str, dict[str, Any]],
    reset: bool,
    build: bool,
    cleanup: bool,
    fail_fast: bool,
    max_workers: int,
    verbose: bool = False,
) -> dict[str, tuple[bool, list[str]]]:
    """
    Run environments using a ProcessPoolExecutor.
    If max_workers is 1, they run sequentially but still in subprocesses for isolation.
    Returns: dict mapping env_name to (success, failed_scenario_names)
    """
    results = {}

    # Calculate max profile name length for aligned logging (+2 for brackets)
    max_label_len = max(len(p) for p in env_scenarios.keys()) + 2 if env_scenarios else 0

    # We use 'spawn' or default context. For simple script execution, default is fine.
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        future_to_profile = {}

        # Submit all tasks
        for profile_name, _ in env_scenarios.items():
            # Build command for subprocess
            cmd = [
                sys.executable,
                "-u",  # Unbuffered output
                "-m",
                "e2e.run_tests",
                "--profile",
                profile_name,
            ]
            if reset:
                cmd.append("--reset")
            if build:
                cmd.append("--build")
            if cleanup:
                cmd.append("--cleanup")
            if fail_fast:
                cmd.append("--fail-fast")
            if verbose:
                cmd.append("--verbose")

            # Determine log prefix/color
            profile_index = list(env_scenarios.keys()).index(profile_name)
            color_code = COLORS[profile_index % len(COLORS)]

            if max_workers > 1:
                print(f"[PARALLEL] Scheduling environment: {profile_name}")

            future = executor.submit(
                run_profile_subprocess, profile_name, cmd, color_code, verbose, max_label_len
            )
            future_to_profile[future] = profile_name

        # Process results as they complete
        for future in as_completed(future_to_profile):
            profile_name = future_to_profile[future]
            try:
                returncode, output = future.result()
                success = returncode == 0
                failed_list = [] if success else [profile_name]

                prefix = "[PARALLEL]" if max_workers > 1 else "[MATRIX]"

                if success:
                    print(f"âœ… {prefix} Environment {profile_name} PASSED")
                else:
                    print(
                        f"âŒ {prefix} Environment {profile_name} FAILED (exit code: {returncode})"
                    )

                results[profile_name] = (success, failed_list)
            except Exception as e:
                print(f"[ERROR] Environment {profile_name} FAILED with exception: {e}")
                results[profile_name] = (False, [f"Environment {profile_name} (exception)"])

    return results


def wait_for_gateway(
    env_name: str, timeout: float = 60.0, interval: float = 1.0, verbose: bool = False
) -> None:
    """
    Waits for the Gateway to be ready by polling its /health endpoint.
    Parity with cli/internal/helpers/wait.go.
    """
    from e2e.runner.env import load_ports

    ports = load_ports(env_name)
    gw_port = ports.get("PORT_GATEWAY_HTTPS")
    if not gw_port:
        if verbose:
            print(f"[WARN] Gateway port not found for {env_name}, skipping readiness wait.")
        return

    url = f"https://localhost:{gw_port}/health"
    if verbose:
        print(f"âžœ Waiting for Gateway readiness at {url}...")

    # Suppress certificate warnings for local dev
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

    deadline = time.time() + timeout
    last_err = None

    while time.time() < deadline:
        try:
            # We use a short timeout for the check itself
            response = requests.get(url, timeout=2.0, verify=False)
            if response.status_code == 200:
                if verbose:
                    print(f"âœ“ Gateway is ready ({response.status_code})")
                return
            last_err = f"Status code {response.status_code}"
        except requests.exceptions.RequestException as e:
            last_err = str(e)

        time.sleep(interval)

    raise RuntimeError(
        f"Gateway failed to start in time ({timeout}s) for {env_name}. Last error: {last_err}"
    )
